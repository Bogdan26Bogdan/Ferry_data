{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a7ccc9-000f-4413-af69-72b98cc1db3f",
   "metadata": {},
   "source": [
    "To install a package over pip so that it works in the notebook you have to run: \n",
    "```bash\n",
    "$ source /opt/tljh/user/bin/activate\n",
    "$ pip install <package name>\n",
    "```\n",
    "and then restart the ipykernal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db132ef9-78ad-49c0-91e9-0e65393ea991",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04f3ccc6-9f24-4625-8bf1-aeba5f17afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ferryFile\n",
    "import port\n",
    "import numpy as np\n",
    "import heapq\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt \n",
    "import matplotlib_inline\n",
    "import itertools\n",
    "from typing import Dict, Set\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38716986-bdf7-47e5-8d58-2cb9a9c217d8",
   "metadata": {},
   "source": [
    "Load the trip times from the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79d204a1-2464-467e-90e5-69126a68912d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SWBTSA': 95, 'SWBSGI': 40, 'HSBNAN': 100, 'TSASWB': 95, 'HSBLNG': 40, 'NANHSB': 100, 'TSASGI': 83, 'SWBFUL': 35, 'DUKTSA': 120, 'TSADUK': 120, 'LNGHSB': 40, 'HSBBOW': 20, 'MILBTW': 25, 'PBBPPH': 390, 'PBBSHW': 45, 'CMXPWR': 90, 'PPHPBB': 360, 'PPHBEC': 1095, 'PPHKLE': 690, 'PPHPOF': 800, 'PPHPPR': 1019, 'PPHSHW': 580, 'ERLSLT': 50, 'FULSWB': 35, 'ALRMCN': 40, 'PWRCMX': 90, 'SLTERL': 50, 'BTWMIL': 25, 'PBBPPR': 614, 'POFBEC': 250, 'SHWPBB': 44, 'SHWBEC': 485, 'SHWPOF': 190, 'KLEPPR': 570, 'PBBBEC': 560, 'PBBKLE': 200, 'PBBPOF': 265, 'POFPBB': 239, 'POFPPH': 780, 'BECPBB': 554, 'BECPOF': 255, 'BECPPH': 1095, 'BECSHW': 480, 'POFSHW': 165, 'SHWPPH': 585, 'KLEPPH': 660, 'PPRPBB': 680, 'KLEPBB': 210, 'PPRKLE': 560, 'PPRPPH': 1110}\n"
     ]
    }
   ],
   "source": [
    "def get_date(file_name: str) -> str:\n",
    "    \"\"\"Time is in the format of YYYY-MM-DD-HH-MM-SS\"\"\"\n",
    "    return (\"-\".join(file_name.split(\"_\")[1].split(\"-\")[0:6])).split(\".\")[0]\n",
    "\n",
    "def get_week_day(time: str) -> str:\n",
    "    \"\"\"Returns 0-6 for monday through sunday for the given date. \n",
    "    Expects the time in YYYY-MM-DD-HH-MM-SS\"\"\"\n",
    "\n",
    "    return datetime.datetime.strptime(time, \"%Y-%m-%d-%H-%M-%S\").weekday()\n",
    "\n",
    "\n",
    "def load_trip_times_and_ports_from_files(directory: str = None) -> Set: \n",
    "    \"\"\"Returns a set of the ports and dictionary of trip times in minutes with the keys being the route codes + the index of the day of the week, \n",
    "    based off the sailing durations in the files.\"\"\"\n",
    "    if directory == None: \n",
    "        directory = \"../BC_Ferries_API_DATA/\"\n",
    "\n",
    "    ports = set()\n",
    "    trip_times = {}\n",
    "    \n",
    "        \n",
    "    for file in os.listdir(directory):\n",
    "        with open(directory + file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            possible_keys = ['routes', 'capacityRoutes', 'nonCapacityRoutes']\n",
    "            #print(get_date(file))\n",
    "            #print(get_week_day(get_date(file)))\n",
    "            week_day = str(get_week_day(get_date(file)))\n",
    "\n",
    "            if str(type(data)) == \"<class 'str'>\":\n",
    "                #print(f\"Type of data is string: {data}\")\n",
    "                #print(\"Continuing.\")\n",
    "                continue\n",
    "\n",
    "            if 'routes' in data.keys():\n",
    "                for route in data['routes']:\n",
    "                    from_port = route['fromTerminalCode']\n",
    "                    to_port = route['toTerminalCode']\n",
    "                    ports.add(from_port)\n",
    "                    ports.add(to_port)\n",
    "\n",
    "                    route_code = route['routeCode']\n",
    "                    assert route_code == from_port + to_port, \"Sanity check that the route code is from_port+to_port\"\n",
    "\n",
    "                    route_code = route_code \n",
    "                    \n",
    "                    trip_time = route['sailingDuration']\n",
    "                    if trip_time == \"\":\n",
    "                        #print(\"Trip time is empty. Continuing\")\n",
    "                        continue\n",
    "                    if \":\" in trip_time: \n",
    "                        #then the format is %H:%M\n",
    "                        hours, minutes = trip_time.split(\":\")\n",
    "                    else:\n",
    "                        #then the format is \"%Hh %%Mm\"\n",
    "                        hours, minutes = trip_time.split(\" \")\n",
    "                        hours = hours.strip(\"h\")\n",
    "                        minutes = minutes.strip(\"m\")\n",
    "\n",
    "                        \n",
    "\n",
    "                    trip_time_in_minutes = 60 * int(hours) + int(minutes)\n",
    "\n",
    "                    # Since the scraped data has multiple routs from one port to another, \n",
    "                    # we are only going to pay attention to the direct route, ie take the min\n",
    "                    if route_code in trip_times.keys():\n",
    "                        if trip_times[route_code] > trip_time_in_minutes:\n",
    "                            trip_times[route_code] = trip_time_in_minutes\n",
    "                    else:\n",
    "                        trip_times[route_code] = trip_time_in_minutes\n",
    "                \n",
    "                    \n",
    "\n",
    "    return ports, trip_times\n",
    "\n",
    "\n",
    "PORT_CODES, TRIP_TIMES = load_trip_times_and_ports_from_files()\n",
    "print(TRIP_TIMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53d307-676b-4b21-89a2-111b8aab3677",
   "metadata": {},
   "source": [
    "Constants and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6968009-62fe-4b4f-a268-40150d9a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIMULATION_TIME = 1000\n",
    "\n",
    "\n",
    "MEAN_TRIP_TIME = 20\n",
    "SD_TRIP_TIME = 1.5\n",
    "MEAN_LOADING_UNLOADING_TIME = 10\n",
    "SD_LOADING_UNLOADING_TIME = 1\n",
    "\n",
    "\n",
    "FERRIES = []\n",
    "SEEDS = [234,4234,324,325543,2,34,546,74,567,568,568,89,364575,342,45,3456473,546,3456473]\n",
    "SEEDS = set(SEEDS)\n",
    "\n",
    "# Programmatically create the port objects for each of the port codes\n",
    "for port_code in PORT_CODES:\n",
    "    globals()[port_code] = port.Port() \n",
    "    globals()[port_code].port_code = port_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724805-1974-4443-b154-7e7fb86b32d0",
   "metadata": {},
   "source": [
    "To generate the cdf we can use np.norm.cdf(val, loc, scale) where location is the mean and scale is the standard deviation. \n",
    "To find the values for a certain percent we can use ppf from scipy stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5603109a-2a3d-4a51-bdeb-40cd5115ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scheduled_departure_and_arrival_times(arrival, departure, trip_length, wanted_on_time_percent: float = 0.85) -> None:\n",
    "    trip_distribution = scipy.stats.norm(loc=MEAN_TRIP_TIME, scale=SD_TRIP_TIME)\n",
    "    loading_unloading_distribution = scipy.stats.norm(loc=MEAN_LOADING_UNLOADING_TIME, scale=SD_LOADING_UNLOADING_TIME)\n",
    "\n",
    "    current_time = 0\n",
    "    for i in range(1, trip_length):\n",
    "        # Add the time to get to the next port\n",
    "        current_time += trip_distribution.ppf(wanted_on_time_percent)\n",
    "\n",
    "        # Set the expected arrival time\n",
    "        arrival[i] = current_time\n",
    "\n",
    "        # Add the time to load and unload at that port\n",
    "        current_time += loading_unloading_distribution.ppf(wanted_on_time_percent)\n",
    "\n",
    "        #Set the expected departure time\n",
    "        departure[i] = current_time\n",
    "\n",
    "    for i in range(len(arrival)):\n",
    "        if type(arrival[i]) not in [int, float]:\n",
    "            arrival[i] = arrival[i].item()\n",
    "    for i in range(len(departure)):\n",
    "        if type(departure[i]) not in [int, float]:\n",
    "            departure[i] = departure[i].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10919f80-1cde-4203-922a-f712c7682b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_arrival = [ 0 for i in range(4)]\n",
    "expected_departure = [0 for i in range(4)]\n",
    "\n",
    "generate_scheduled_departure_and_arrival_times(expected_arrival, expected_departure, 4, 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b345d7-3c54-4708-a182-a0467e7ea983",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPF value: 10\n",
      "PPF value: 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m port_code \u001b[38;5;129;01min\u001b[39;00m PORT_CODES:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, MAX_SIMULATION_TIME, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m---> 31\u001b[0m         LOADING_AND_UNLOADING_TIMES[\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[port_code] \u001b[38;5;241m+\u001b[39m time] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mnormal(MEAN_LOADING_UNLOADING_TIME, SD_LOADING_UNLOADING_TIME)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#Create a ferry object\u001b[39;00m\n\u001b[0;32m     36\u001b[0m QNW \u001b[38;5;241m=\u001b[39m ferryFile\u001b[38;5;241m.\u001b[39mFerry()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_to_count_as_late = 0\n",
    "lates_for_each = {}\n",
    "for ppf_value in range(1, 100, 1):\n",
    "    if (ppf_value % 10 == 0):\n",
    "        print(f\"PPF value: {ppf_value}\")\n",
    "    ppf_value = ppf_value / 100\n",
    "    lates_for_each[ppf_value] = []\n",
    "\n",
    "    expected_arrival = [ 0 for i in range(4)]\n",
    "    expected_departure = [0 for i in range(4)]\n",
    "    generate_scheduled_departure_and_arrival_times(expected_arrival, expected_departure, 4, ppf_value)\n",
    "    \n",
    "    for seed_value in range(100):\n",
    "        np.random.seed(seed_value)\n",
    "        # Most of the trips take the same amount of time independent of the ferry \n",
    "        # Makes every port to every other port take 20ish minutes. \n",
    "        # Times are made to be the same independent of direction.\n",
    "        TRIP_TIMES = {}\n",
    "        for port in PORT_CODES:\n",
    "            for other_port in [i for i in PORT_CODES if i != port]:\n",
    "                if other_port + port in TRIP_TIMES.keys():\n",
    "                    TRIP_TIMES[port+other_port] = TRIP_TIMES[other_port+port]\n",
    "                else:\n",
    "                    TRIP_TIMES[port + other_port] = np.random.normal(MEAN_TRIP_TIME, SD_TRIP_TIME)\n",
    "    \n",
    "        # Set the loading and unloading times\n",
    "        # Port + Current_time for every 5 step increment\n",
    "        LOADING_AND_UNLOADING_TIMES = {}\n",
    "        for port_code in PORT_CODES:\n",
    "            for time in range(0, MAX_SIMULATION_TIME, 5):\n",
    "                LOADING_AND_UNLOADING_TIMES[globals()[port_code] + time] = np.random.normal(MEAN_LOADING_UNLOADING_TIME, SD_LOADING_UNLOADING_TIME)\n",
    "        \n",
    "\n",
    "        \n",
    "        #Create a ferry object\n",
    "        QNW = ferryFile.Ferry()\n",
    "        QNW.ferry_name = \"Queen of New Westminster\"\n",
    "        QNW.ferry_code = \"QNW\"\n",
    "        QNW.ferry_capacity = 150\n",
    "        QNW.ferry_route = [TSA, SWB, TSA]\n",
    "        QNW.ferry_trip_time = TRIP_TIMES\n",
    "        QNW.set_expected_departure_times(expected_departure)\n",
    "        QNW.set_expected_arrival_times(expected_arrival)\n",
    "        QNW.loading_unloading_time = LOADING_AND_UNLOADING_TIMES\n",
    "        QNW.trips_required = 1\n",
    "        \n",
    "        QNW.trips_completed = 0\n",
    "        QNW.ferry_current_port_index = 0\n",
    "\n",
    "        QNW.print_stats_at_end = False\n",
    "\n",
    "        #QNW.validate_arrival_and_departure_times()\n",
    "    \n",
    "        current_time =0\n",
    "        event_queue = []\n",
    "        heapq.heapify(event_queue)\n",
    "        heapq.heappush(event_queue, (current_time, QNW))\n",
    "        \n",
    "    \n",
    "        current_event = heapq.heappop(event_queue)\n",
    "        while current_event[1].next_function != None:\n",
    "            current_time = current_event[0]\n",
    "    \n",
    "            next_event_time = current_event[1].next_function(current_time)\n",
    "            heapq.heappush(event_queue, (next_event_time, current_event[1]))\n",
    "    \n",
    "            current_event = heapq.heappop(event_queue)\n",
    "\n",
    "        lates_for_each[ppf_value].append(QNW.total_times_late(time_to_count_as_late))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(lates_for_each)\n",
    "for key in lates_for_each.keys():\n",
    "    data = lates_for_each[key]\n",
    "    times_late_to_arrive = [i[0] for i in data]\n",
    "    times_late_to_depart = [i[1] for i in data]\n",
    "\n",
    "    times_late_to_arrive = sum(times_late_to_arrive) / len(times_late_to_arrive)\n",
    "    times_late_to_depart = sum(times_late_to_depart) / len(times_late_to_depart)\n",
    "\n",
    "    lates_for_each[key] = [times_late_to_arrive, times_late_to_depart]\n",
    "#print(lates_for_each)\n",
    "\n",
    "plt.bar(lates_for_each.keys(), [lates_for_each[key][0] for key in lates_for_each.keys()], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb70b9a-7c2d-444f-8364-a56dbc120f52",
   "metadata": {},
   "source": [
    "Use pandas to load data from csv - Keep in mind to run this locally, and not upload the files to github or the jupyter server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "673b8133-e6c4-438c-a7bb-38432960cff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date_key</th>\n",
       "      <th>day_of_the_week</th>\n",
       "      <th>route_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-04-02</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-03</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-04-04</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-04-05</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-04-06</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2024-04-07</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2024-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2024-04-09</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2024-04-10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2024-04-11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2024-04-13</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2024-04-14</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2024-04-15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2024-04-18</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2024-04-19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2024-04-24</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2024-04-27</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      date_key  day_of_the_week  route_id\n",
       "1   2024-04-02                1         5\n",
       "2   2024-04-03                2         5\n",
       "3   2024-04-04                3         5\n",
       "4   2024-04-05                4         5\n",
       "5   2024-04-06                5         5\n",
       "6   2024-04-07                6         5\n",
       "7   2024-04-08                0         5\n",
       "8   2024-04-09                1         5\n",
       "9   2024-04-10                2         5\n",
       "10  2024-04-11                3         5\n",
       "12  2024-04-13                5         5\n",
       "13  2024-04-14                6         5\n",
       "14  2024-04-15                0         5\n",
       "17  2024-04-18                3         5\n",
       "18  2024-04-19                4         5\n",
       "23  2024-04-24                2         5\n",
       "26  2024-04-27                5         5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These are some pandas examples\n",
    "data_file = None\n",
    "\n",
    "#Load data from file\n",
    "#all_data = pd.read_csv(file)\n",
    "data = {\"date_key\": [\"2024-04-0\" + str(i) for i in range(1,10)]}\n",
    "for i in range(10,28):\n",
    "    data[\"date_key\"].append(\"2024-04-\" + str(i))\n",
    "all_data = pd.DataFrame(data = data)\n",
    "\n",
    "#Seperate data by day of the week\n",
    "all_data['day_of_the_week'] = [datetime.datetime.strptime(i, \"%Y-%m-%d\").weekday() for i in all_data['date_key']]\n",
    "\n",
    "#Add route code\n",
    "all_data[\"route_id\"] = np.random.choice([5,9], all_data.shape[0])\n",
    "\n",
    "all_data.loc[all_data[\"route_id\"] == 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c48ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the year for the data from the file name\n",
    "def get_year(file_name: str) -> str:\n",
    "    print(file_name)\n",
    "    file_name = file_name.split(\"fy\")[1].strip(\".csv\").strip(\"fy\")\n",
    "    if \"_\" in file_name:\n",
    "        return file_name.split(\"_\")[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d71bde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds a date key to the data  to make it easier to filter\n",
    "def add_date_key(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    #If the data set is analyze or event then there is a \"date_key\" column exclusive of the end date\n",
    "    if \"date_key\" in data.columns: \n",
    "        #Date is in the format of YYYY-MM-DD\n",
    "        data[\"date_key\"] = pd.to_datetime(data[\"date_key\"])\n",
    "\n",
    "        date = (data.loc[data[\"sailing_id\"] == \"LONG202304071800\"][\"date_key\"].values[0])\n",
    "        print(type(date))\n",
    "        print(date.astype(str)[:10])\n",
    "        \n",
    "\n",
    "    #If it is injest then there is a \"Sched Dept Ts\" column\n",
    "    elif \"Sched Dept Ts\" in data.columns: \n",
    "        #Date is the format of YYYY-MM-DD-HH-MM-SS\n",
    "        data[\"date_key\"] = pd.to_datetime(data[\"Sched Dept Ts\"])\n",
    "\n",
    "    #Throw an error to show that we have a data set without the proper columns\n",
    "    else: \n",
    "        print(\"A data set was passed that did not follow any of the formats\")\n",
    "        assert(False)\n",
    "        \n",
    "\n",
    "def filter_by_dates(data: pd.DataFrame, start_date: str | np.datetime64, end_date: str | np.datetime64) -> pd.DataFrame:\n",
    "    # Filter the data set by the start and end date and return the new data set\n",
    "    if type(start_date) is str:\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    if type(end_date) is str:\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "\n",
    "\n",
    "    assert(start_date <= end_date), \"Start date must be before end date\"\n",
    "\n",
    "    new_data = data.loc[data[\"date_key\"] >= start_date]\n",
    "    new_data = new_data.loc[new_data[\"date_key\"] < end_date]\n",
    "\n",
    "    return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fbadd73-9ccc-47d8-bbf0-ff776207ee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Bogdan\\AppData\\Local\\Temp\\ipykernel_23580\\1878275666.py:9: DtypeWarning: Columns (0,1,2,7,9,10,11,12,13,14,15,16,17,18,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_analyze_data = pd.concat(map(pd.read_csv, [ANALYZE_FOLDER + \"/\" + file for file in files]))\n",
      "C:\\Users\\Bogdan\\AppData\\Local\\Temp\\ipykernel_23580\\1878275666.py:9: DtypeWarning: Columns (0,1,2,7,9,10,11,12,13,14,15,16,17,18,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_analyze_data = pd.concat(map(pd.read_csv, [ANALYZE_FOLDER + \"/\" + file for file in files]))\n",
      "C:\\Users\\Bogdan\\AppData\\Local\\Temp\\ipykernel_23580\\1878275666.py:9: DtypeWarning: Columns (0,1,2,7,9,10,11,12,13,14,15,16,17,18,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_analyze_data = pd.concat(map(pd.read_csv, [ANALYZE_FOLDER + \"/\" + file for file in files]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.datetime64'>\n",
      "2023-04-07\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert(load_dotenv())\n",
    "\n",
    "ANALYZE_FOLDER = os.getenv(\"SAILING_ANALYZE_FOLDER\")\n",
    "INGEST_FOLDER = os.getenv(\"SAILING_INGEST_FOLDER\")\n",
    "EVENT_FOLDER = os.getenv(\"SAILING_EVENT_FOLDER\")\n",
    "\n",
    "files = os.listdir(ANALYZE_FOLDER) \n",
    "all_analyze_data = pd.concat(map(pd.read_csv, [ANALYZE_FOLDER + \"/\" + file for file in files]))\n",
    "all_analyze_data.dropna(how='all', inplace=True, ignore_index=True)\n",
    "\n",
    "def convert_to_epoch(date: str):\n",
    "    dt_obj = datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M\")\n",
    "    return int(dt_obj.timestamp())\n",
    "\n",
    "all_analyze_data[\"arrival_actual_datetime\"] = all_analyze_data[\"arrival_actual_datetime\"].apply(convert_to_epoch)\n",
    "all_analyze_data[\"departure_actual_datetime\"] = all_analyze_data[\"departure_actual_datetime\"].apply(convert_to_epoch)\n",
    "\n",
    "all_analyze_data[\"sailing_time\"] = (all_analyze_data[\"arrival_actual_datetime\"] - all_analyze_data[\"departure_actual_datetime\"])/60\n",
    "add_date_key(all_analyze_data)\n",
    "assert(all_analyze_data.loc[all_analyze_data[\"sailing_time\"] <= 0].empty) #Sanity check that none of the boats are time travellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72499e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_analyze_data[\"date_key\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sailing ID: SWB202207310555 In Port Time(m): 9.0\n",
      "Sailing ID: SWB202207310620 In Port Time(m): 4.0\n",
      "Sailing ID: SWB202207310915 In Port Time(m): 8.0\n",
      "Sailing ID: SWB202207311000 In Port Time(m): 3.0\n",
      "Sailing ID: SWB202207311410 In Port Time(m): 14.0\n",
      "Sailing ID: SWB202207311420 In Port Time(m): 14.0\n",
      "Sailing ID: LONG202211251800 In Port Time(m): 12.0\n",
      "Sailing ID: LONG202212021800 In Port Time(m): 14.0\n",
      "Sailing ID: LONG202212091800 In Port Time(m): 10.0\n",
      "Sailing ID: LONG202212161800 In Port Time(m): 4.0\n",
      "Sailing ID: LONG202212301800 In Port Time(m): 9.0\n",
      "Sailing ID: LONG202301061800 In Port Time(m): 8.0\n",
      "Sailing ID: LONG202301131800 In Port Time(m): 21.0\n",
      "Sailing ID: LONG202301201800 In Port Time(m): 2.0\n",
      "Sailing ID: LONG202301271800 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202204021650 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202204161650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202204231650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202204301650 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202205071650 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202205141650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202205211650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202205281650 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202206041650 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202206111650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202206181650 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202206251650 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202207031150 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202207101150 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202207171150 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202207241150 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202207311150 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202208071150 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202208141150 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202208211150 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202208281150 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202209041150 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202209101650 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202209171650 In Port Time(m): 14.0\n",
      "Sailing ID: SAT202209241650 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202210011650 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202210081650 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202212111130 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212191626 In Port Time(m): 4.0\n",
      "Sailing ID: SAT202212210616 In Port Time(m): 13.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202211202100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211212200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211222200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211232200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211242200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211252155 In Port Time(m): -1\n",
      "Sailing ID: SAT202211261155 In Port Time(m): 114.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202211262105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211272100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211282200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211292200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202211302200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212012200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212022155 In Port Time(m): -1\n",
      "Sailing ID: SAT202212031155 In Port Time(m): 95.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202212032105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212042100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212052200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212062200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212072200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212082200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212092155 In Port Time(m): -1\n",
      "Sailing ID: SAT202212101155 In Port Time(m): 97.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202212102105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212112100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212122200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212132200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212142200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212152200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212162155 In Port Time(m): -1\n",
      "Sailing ID: SAT202212171155 In Port Time(m): 99.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202212172105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212182100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212192200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212212200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212222200 In Port Time(m): -1\n",
      "Sailing ID: SAT202212241155 In Port Time(m): 109.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202212242105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212252100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212262100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212272200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212282200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212292200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202212302155 In Port Time(m): -1\n",
      "Sailing ID: SAT202212311155 In Port Time(m): 95.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202212312105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301012100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301022100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301032200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301042200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301052200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301062155 In Port Time(m): -1\n",
      "Sailing ID: SAT202301071155 In Port Time(m): 110.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202301072105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301082100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301092200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301102200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301112200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301122200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301132155 In Port Time(m): -1\n",
      "Sailing ID: SAT202301141155 In Port Time(m): 89.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202301142105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301152100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301162200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301172200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301182200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301192200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301202155 In Port Time(m): -1\n",
      "Sailing ID: SAT202301211155 In Port Time(m): 86.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202301212105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301222100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301232200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301242200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301252200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301272155 In Port Time(m): -1\n",
      "Sailing ID: SAT202301281155 In Port Time(m): 91.0\n",
      "No data for the day\n",
      "Sailing ID: SAT202301282105 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301292100 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301302200 In Port Time(m): -1\n",
      "No data for the day\n",
      "Sailing ID: SAT202301312200 In Port Time(m): -1\n",
      "Sailing ID: SAT202211200940 In Port Time(m): 50.0\n",
      "Sailing ID: SAT202211201730 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202211210615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202211211025 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202211211625 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202211220615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202211221025 In Port Time(m): 16.0\n",
      "Sailing ID: SAT202211221625 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202211230615 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202211231025 In Port Time(m): 17.0\n",
      "Sailing ID: SAT202211231625 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202211240615 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202211241025 In Port Time(m): 18.0\n",
      "Sailing ID: SAT202211241625 In Port Time(m): 39.0\n",
      "Sailing ID: SAT202211250615 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202211251025 In Port Time(m): 15.0\n",
      "Sailing ID: SAT202211251625 In Port Time(m): 17.0\n",
      "Sailing ID: SAT202211260955 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202211270940 In Port Time(m): 36.0\n",
      "Sailing ID: SAT202211271730 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202211280615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202211281025 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202211281625 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202211290615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202211291025 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202211291625 In Port Time(m): 24.0\n",
      "Sailing ID: SAT202211300615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202211301025 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202211301625 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202212010615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212011025 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202212011625 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212020615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212021025 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212021625 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202212030955 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202212040940 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212041730 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202212050615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212051025 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202212051625 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212060615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212061025 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202212061625 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212070615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212071025 In Port Time(m): 24.0\n",
      "Sailing ID: SAT202212071625 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212080615 In Port Time(m): 4.0\n",
      "Sailing ID: SAT202212081025 In Port Time(m): 26.0\n",
      "Sailing ID: SAT202212081625 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212090615 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202212091025 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202212091625 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202212100955 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202212110740 In Port Time(m): 26.0\n",
      "Sailing ID: SAT202212110940 In Port Time(m): 3.0\n",
      "Sailing ID: SAT202212111730 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212120616 In Port Time(m): 21.0\n",
      "Sailing ID: SAT202212121025 In Port Time(m): 3.0\n",
      "Sailing ID: SAT202212121625 In Port Time(m): 26.0\n",
      "Sailing ID: SAT202212130615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212131025 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202212131625 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212140615 In Port Time(m): 4.0\n",
      "Sailing ID: SAT202212141025 In Port Time(m): 16.0\n",
      "Sailing ID: SAT202212141625 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212150615 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212151025 In Port Time(m): 23.0\n",
      "Sailing ID: SAT202212151625 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202212160615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212161025 In Port Time(m): 21.0\n",
      "Sailing ID: SAT202212161625 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202212170955 In Port Time(m): 14.0\n",
      "Sailing ID: SAT202212180940 In Port Time(m): 21.0\n",
      "Sailing ID: SAT202212181730 In Port Time(m): 12.0\n",
      "Sailing ID: SAT202212190615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212191025 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202212211025 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202212211625 In Port Time(m): 15.0\n",
      "Sailing ID: SAT202212220615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212221025 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212221625 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202212231025 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212240955 In Port Time(m): 13.0\n",
      "Sailing ID: SAT202212250940 In Port Time(m): 4.0\n",
      "Sailing ID: SAT202212251730 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202212260615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212261025 In Port Time(m): 20.0\n",
      "Sailing ID: SAT202212261730 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212270615 In Port Time(m): 4.0\n",
      "Sailing ID: SAT202212271025 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202212271625 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212280615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212281025 In Port Time(m): 15.0\n",
      "Sailing ID: SAT202212281625 In Port Time(m): 7.0\n",
      "Sailing ID: SAT202212290615 In Port Time(m): 8.0\n",
      "Sailing ID: SAT202212291025 In Port Time(m): 16.0\n",
      "Sailing ID: SAT202212291625 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202212300615 In Port Time(m): 5.0\n",
      "Sailing ID: SAT202212301025 In Port Time(m): 10.0\n",
      "Sailing ID: SAT202212301625 In Port Time(m): 18.0\n",
      "Sailing ID: SAT202212310955 In Port Time(m): 20.0\n",
      "Sailing ID: SAT202301010940 In Port Time(m): 47.0\n",
      "Sailing ID: SAT202301011730 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202301020615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202301021025 In Port Time(m): 11.0\n",
      "Sailing ID: SAT202301021730 In Port Time(m): 9.0\n",
      "Sailing ID: SAT202301030615 In Port Time(m): 6.0\n",
      "Sailing ID: SAT202301031025 In Port Time(m): 17.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m all_analyze_data\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m     40\u001b[0m     sailing_id \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msailing_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m     in_port_time \u001b[38;5;241m=\u001b[39m \u001b[43mget_in_port_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_analyze_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msailing_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSailing ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msailing_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m In Port Time(m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00min_port_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mget_in_port_time\u001b[1;34m(data, sailing_id)\u001b[0m\n\u001b[0;32m      4\u001b[0m sailing_info \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msailing_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m sailing_id]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sailing_info\u001b[38;5;241m.\u001b[39mindex) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should only be one sailing id in the data set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m data_for_the_day \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_by_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msailing_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msailing_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimedelta64\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mD\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m arrival_port_id \u001b[38;5;241m=\u001b[39m sailing_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marr_port_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m#Filter by port\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 35\u001b[0m, in \u001b[0;36mfilter_by_dates\u001b[1;34m(data, start_date, end_date)\u001b[0m\n\u001b[0;32m     29\u001b[0m     end_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mstrptime(end_date, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(start_date \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end_date), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart date must be before end date\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m new_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mloc[\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdate_key\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m]\n\u001b[0;32m     36\u001b[0m new_data \u001b[38;5;241m=\u001b[39m new_data\u001b[38;5;241m.\u001b[39mloc[new_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdate_key\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m end_date]\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_data\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:6119\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   6116\u001b[0m lvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   6117\u001b[0m rvalues \u001b[38;5;241m=\u001b[39m extract_array(other, extract_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 6119\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(res_values, name\u001b[38;5;241m=\u001b[39mres_name)\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py:330\u001b[0m, in \u001b[0;36mcomparison_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLengths must match to compare\u001b[39m\u001b[38;5;124m\"\u001b[39m, lvalues\u001b[38;5;241m.\u001b[39mshape, rvalues\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    323\u001b[0m         )\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m    326\u001b[0m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m lvalues\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[0;32m    328\u001b[0m ):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[1;32m--> 330\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator\u001b[38;5;241m.\u001b[39mne:\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\ops\\common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arraylike.py:60\u001b[0m, in \u001b[0;36mOpsMixin.__ge__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__ge__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bogdan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\arrays\\datetimelike.py:1022\u001b[0m, in \u001b[0;36mDatetimeLikeArrayMixin._cmp_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   1020\u001b[0m other_vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unbox(other)\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# GH#37462 comparison on i8 values is almost 2x faster than M8/m8\u001b[39;00m\n\u001b[1;32m-> 1022\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ndarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_vals\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1024\u001b[0m o_mask \u001b[38;5;241m=\u001b[39m isna(other)\n\u001b[0;32m   1025\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_isnan \u001b[38;5;241m|\u001b[39m o_mask\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Get the inport time for the vessel\n",
    "def get_in_port_time(data: pd.DataFrame, sailing_id: str) -> int:\n",
    "    #Get the information for the sailing id and data from the day\n",
    "    sailing_info = data.loc[data[\"sailing_id\"] == sailing_id]\n",
    "    assert len(sailing_info.index) == 1, \"There should only be one sailing id in the data set\"\n",
    "    data_for_the_day = filter_by_dates(data, sailing_info[\"date_key\"].values[0], sailing_info[\"date_key\"].values[0] + np.timedelta64(1, 'D'))\n",
    "    \n",
    "    arrival_port_id = sailing_info[\"arr_port_id\"].values[0]\n",
    "\n",
    "    #Filter by port\n",
    "    data_for_the_day = data_for_the_day.loc[data_for_the_day[\"dep_port_id\"] == arrival_port_id]\n",
    "    if len(data_for_the_day.index) == 0:\n",
    "        print(\"No data for the day\")\n",
    "        return -1\n",
    "    #Filter by time\n",
    "    sailing_arrival = sailing_info[\"arrival_actual_datetime\"].values[0]\n",
    "    data_for_the_day = data_for_the_day.loc[data_for_the_day[\"departure_actual_datetime\"] > sailing_arrival]\n",
    "    if len(data_for_the_day.index) == 0:\n",
    "        print(\"No data for the day\")\n",
    "        return -1\n",
    "    #Get what should have been the arrival\n",
    "    time_wanted = data_for_the_day[\"departure_actual_datetime\"].min()\n",
    "    arrival = data_for_the_day.loc[data_for_the_day[\"departure_actual_datetime\"] == time_wanted]\n",
    "\n",
    "\n",
    "    arri_time = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=int(sailing_info[\"arrival_actual_datetime\"].values[0].astype(str)))\n",
    "    dept_time = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=int(arrival[\"departure_actual_datetime\"].values[0].astype(str)))\n",
    "\n",
    "\n",
    "\n",
    "    # Compute difference\n",
    "    time_diff = dept_time - arri_time\n",
    "    return time_diff.total_seconds() / 60\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for index, row in all_analyze_data.iterrows():\n",
    "    sailing_id = row[\"sailing_id\"]\n",
    "    row[\"in_port_time_at_destination\"] = get_in_port_time(all_analyze_data, sailing_id)\n",
    "    print(f\"Sailing ID: {sailing_id} In Port Time(m): {row[\"in_port_time_at_destination\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf917e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1236d508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24755   2023-04-07\n",
       "Name: date_key, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = all_analyze_data.loc[all_analyze_data[\"sailing_id\"] == \"OB202304071815\"]\n",
    "x[\"date_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(EVENT_FOLDER)\n",
    "eventData = pd.concat(map(pd.read_csv, [EVENT_FOLDER + \"/\" + file for file in files])) #Read all the files and concatenate them into one dataframe\n",
    "eventData.dropna(how=\"all\", inplace=True, ignore_index=True)\n",
    "\n",
    "arr_ports = [int(i) for i in  eventData[\"arr_port_id\"].unique()]\n",
    "dep_ports = [int(i) for i in  eventData[\"dep_port_id\"].unique()]\n",
    "\n",
    "ports = set(arr_ports + dep_ports)\n",
    "\n",
    "portID_to_port_map = {}\n",
    "for port in ports: \n",
    "    #Check arrivals \n",
    "    port_abb_arr = eventData.loc[eventData[\"arr_port_id\"] == port][\"arr_port_abb\"].unique()\n",
    "\n",
    "    #Check departures\n",
    "    port_abb_dep = eventData.loc[eventData[\"dep_port_id\"] == port][\"dep_port_abb\"].unique()\n",
    "\n",
    "    #Verify that one the abbreviation was found for atleast 1 and if it was found for only one assign it to both.\n",
    "    if len(port_abb_dep) == 0 and len(port_abb_arr) == 0: \n",
    "        print(f\"Could not find the abbreviation for {port}.\")\n",
    "        continue\n",
    "    elif len(port_abb_dep) == 0: \n",
    "        port_abb_dep = port_abb_arr\n",
    "    elif len(port_abb_arr) == 0:\n",
    "        port_abb_arr = port_abb_dep\n",
    "\n",
    "    port_abb_arr = port_abb_arr[0]\n",
    "    port_abb_dep = port_abb_dep[0]\n",
    "    \n",
    "    \n",
    "    assert(port_abb_dep == port_abb_arr)\n",
    "\n",
    "    if port in portID_to_port_map.keys():\n",
    "        assert(portID_to_port_map[port] == port_abb_arr) #Sanity check\n",
    "    else: \n",
    "        portID_to_port_map[port] = port_abb_arr\n",
    "\n",
    "    \n",
    "def portID_to_abb(portID:int | str) -> str: \n",
    "    if type(portID) is str: \n",
    "        portID = int(portID)\n",
    "\n",
    "    return portID_to_port_map[portID]\n",
    "\n",
    "portID_to_port_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = \"figures\"\n",
    "\n",
    "#Seperate data by route\n",
    "Route5_data = all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 5)]\n",
    "Route9_data = all_analyze_data.loc[all_analyze_data[\"route_id\"] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_port_from_route(route_data, route_leg_id, arrival: bool = False, departure: bool = False):\n",
    "    if arrival and departure:\n",
    "        raise ValueError(\"Both arrival and departure cannot be true\")\n",
    "    if not arrival and not departure:\n",
    "        raise ValueError(\"Either arrival or departure must be true\")\n",
    "    if arrival:\n",
    "        key = \"arr_port_id\"\n",
    "    else:\n",
    "        assert(departure)\n",
    "        key = \"dep_port_id\"\n",
    "\n",
    "\n",
    "    port_id = route_data[route_data[\"route_leg_id\"] == route_leg_id][key].unique()\n",
    "    assert(len(port_id) == 1)\n",
    "    port_id = port_id[0]\n",
    "    return port_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e90867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make histograms for each of the legs with all of the vessels to see the general distribution of sailing times\n",
    "def make_histograms_for_legs(route_data: pd.DataFrame) -> None:\n",
    "    route_vessels = route_data[\"vessel_id\"].unique()\n",
    "    route_legs = route_data[\"route_leg_id\"].unique()\n",
    "\n",
    "\n",
    "    for route_leg_id in route_legs: \n",
    "        #Get the arrival and departure port ids\n",
    "        arrival_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, arrival=True))))\n",
    "        departure_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, departure=True))))\n",
    "\n",
    "        route_figure = plt.figure()\n",
    "        plt.title(f\"Route {route_leg_id.split('-')[0]}, Leg {arrival_port_id} to {departure_port_id}\")\n",
    "        plt.xlabel(\"Sailing Time (m)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        #Get the min x and max x for the histogram\n",
    "        min_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].min()\n",
    "        max_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].max()\n",
    "\n",
    "        \n",
    "        for vessel in route_vessels: \n",
    "            #verify the vessel has sailed on the route otherwise skip\n",
    "            if route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)].empty:\n",
    "                continue\n",
    "\n",
    "            #plot the histogram\n",
    "            route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)][\"sailing_time\"].hist(bins=25, alpha=0.3, label=f\"Vessel {vessel}\", range=[min_x, max_x])\n",
    "\n",
    "        #format the plot\n",
    "        plt.grid(False)\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"{FIGURES_DIR}/R{route_leg_id.split('-')[0]}_{departure_port_id + arrival_port_id}_histogram.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "make_histograms_for_legs(Route5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plots for the sailing times on each route for each vessel to check if the timing is consistent across vessels.\n",
    "def make_subplot_histograms_for_legs(route_data: pd.DataFrame) -> None:\n",
    "    route_vessels = route_data[\"vessel_id\"].unique()\n",
    "    route_legs = route_data[\"route_leg_id\"].unique()\n",
    "\n",
    "    for route_leg_id in route_legs: \n",
    "        #Get the arrival, departure port ids and the route id\n",
    "        arrival_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, arrival=True))))\n",
    "        departure_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, departure=True))))\n",
    "        route_id = route_leg_id.split(\"-\")[0]\n",
    "\n",
    "        #Create the shared plot\n",
    "        route_figure = plt.figure()\n",
    "        plt.suptitle(f\"Route {route_id}, Leg {departure_port_id} to {arrival_port_id}\")\n",
    "\n",
    "        #Get the min x and max x for the histogram\n",
    "        min_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].min()\n",
    "        max_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].max()\n",
    "\n",
    "        for vessel in route_vessels: \n",
    "            #verify the vessel has sailed on the route\n",
    "            if route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)].empty:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #Determine the subplot location\n",
    "            unique_vessels_for_route = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"vessel_id\"].nunique()\n",
    "            index_array_for_vessel_on_route = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"vessel_id\"].unique()\n",
    "            plt.subplot(1 if unique_vessels_for_route <= 3 else 2, \n",
    "                        unique_vessels_for_route if unique_vessels_for_route <= 3 else 3, \n",
    "                        np.where(index_array_for_vessel_on_route == vessel)[0][0] + 1)\n",
    "            \n",
    "            #Plot the data\n",
    "            route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)][\"sailing_time\"].hist(\n",
    "                bins=20, alpha=0.5, label=f\"Vessel {vessel}\", range=[min_x, max_x])\n",
    "\n",
    "            #Set the title, labels, and grid\n",
    "            plt.title(f\"Vessel {vessel}\")\n",
    "            plt.xlabel(\"Sailing Time (m)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.grid(False)\n",
    "            \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{FIGURES_DIR}/Route{route_id}_SubPlots_{departure_port_id+arrival_port_id}.png\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "make_subplot_histograms_for_legs(Route5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df61c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 9) & (all_analyze_data[\"route_leg_id\"].apply(lambda x: x.split(\"-\")[0]) == \"5\")]\n",
    "all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 5) & (all_analyze_data[\"route_leg_id\"].apply(lambda x: x.split(\"-\")[0]) == \"9\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7295bc",
   "metadata": {},
   "source": [
    "##Simple sim without ferries mattering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d7ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
