{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a7ccc9-000f-4413-af69-72b98cc1db3f",
   "metadata": {},
   "source": [
    "To install a package over pip so that it works in the notebook you have to run: \n",
    "```bash\n",
    "$ source /opt/tljh/user/bin/activate\n",
    "$ pip install <package name>\n",
    "```\n",
    "and then restart the ipykernal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db132ef9-78ad-49c0-91e9-0e65393ea991",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3ccc6-9f24-4625-8bf1-aeba5f17afad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ferryFile\n",
    "import port\n",
    "import numpy as np\n",
    "import heapq\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt \n",
    "import matplotlib_inline\n",
    "import itertools\n",
    "from typing import Dict, Set\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38716986-bdf7-47e5-8d58-2cb9a9c217d8",
   "metadata": {},
   "source": [
    "Load the trip times from the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d204a1-2464-467e-90e5-69126a68912d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_date(file_name: str) -> str:\n",
    "    \"\"\"Time is in the format of YYYY-MM-DD-HH-MM-SS\"\"\"\n",
    "    return (\"-\".join(file_name.split(\"_\")[1].split(\"-\")[0:6])).split(\".\")[0]\n",
    "\n",
    "def get_week_day(time: str) -> str:\n",
    "    \"\"\"Returns 0-6 for monday through sunday for the given date. \n",
    "    Expects the time in YYYY-MM-DD-HH-MM-SS\"\"\"\n",
    "\n",
    "    return datetime.datetime.strptime(time, \"%Y-%m-%d-%H-%M-%S\").weekday()\n",
    "\n",
    "\n",
    "def load_trip_times_and_ports_from_files(directory: str = None) -> Set: \n",
    "    \"\"\"Returns a set of the ports and dictionary of trip times in minutes with the keys being the route codes + the index of the day of the week, \n",
    "    based off the sailing durations in the files.\"\"\"\n",
    "    if directory == None: \n",
    "        directory = \"../BC_Ferries_API_DATA/\"\n",
    "\n",
    "    ports = set()\n",
    "    trip_times = {}\n",
    "    \n",
    "        \n",
    "    for file in os.listdir(directory):\n",
    "        with open(directory + file, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            possible_keys = ['routes', 'capacityRoutes', 'nonCapacityRoutes']\n",
    "            #print(get_date(file))\n",
    "            #print(get_week_day(get_date(file)))\n",
    "            week_day = str(get_week_day(get_date(file)))\n",
    "\n",
    "            if str(type(data)) == \"<class 'str'>\":\n",
    "                #print(f\"Type of data is string: {data}\")\n",
    "                #print(\"Continuing.\")\n",
    "                continue\n",
    "\n",
    "            if 'routes' in data.keys():\n",
    "                for route in data['routes']:\n",
    "                    from_port = route['fromTerminalCode']\n",
    "                    to_port = route['toTerminalCode']\n",
    "                    ports.add(from_port)\n",
    "                    ports.add(to_port)\n",
    "\n",
    "                    route_code = route['routeCode']\n",
    "                    assert route_code == from_port + to_port, \"Sanity check that the route code is from_port+to_port\"\n",
    "\n",
    "                    route_code = route_code \n",
    "                    \n",
    "                    trip_time = route['sailingDuration']\n",
    "                    if trip_time == \"\":\n",
    "                        #print(\"Trip time is empty. Continuing\")\n",
    "                        continue\n",
    "                    if \":\" in trip_time: \n",
    "                        #then the format is %H:%M\n",
    "                        hours, minutes = trip_time.split(\":\")\n",
    "                    else:\n",
    "                        #then the format is \"%Hh %%Mm\"\n",
    "                        hours, minutes = trip_time.split(\" \")\n",
    "                        hours = hours.strip(\"h\")\n",
    "                        minutes = minutes.strip(\"m\")\n",
    "\n",
    "                        \n",
    "\n",
    "                    trip_time_in_minutes = 60 * int(hours) + int(minutes)\n",
    "\n",
    "                    # Since the scraped data has multiple routs from one port to another, \n",
    "                    # we are only going to pay attention to the direct route, ie take the min\n",
    "                    if route_code in trip_times.keys():\n",
    "                        if trip_times[route_code] > trip_time_in_minutes:\n",
    "                            trip_times[route_code] = trip_time_in_minutes\n",
    "                    else:\n",
    "                        trip_times[route_code] = trip_time_in_minutes\n",
    "                \n",
    "                    \n",
    "\n",
    "    return ports, trip_times\n",
    "\n",
    "\n",
    "PORT_CODES, TRIP_TIMES = load_trip_times_and_ports_from_files()\n",
    "print(TRIP_TIMES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53d307-676b-4b21-89a2-111b8aab3677",
   "metadata": {},
   "source": [
    "Constants and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6968009-62fe-4b4f-a268-40150d9a9644",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SIMULATION_TIME = 1000\n",
    "\n",
    "\n",
    "MEAN_TRIP_TIME = 20\n",
    "SD_TRIP_TIME = 1.5\n",
    "MEAN_LOADING_UNLOADING_TIME = 10\n",
    "SD_LOADING_UNLOADING_TIME = 1\n",
    "\n",
    "\n",
    "FERRIES = []\n",
    "SEEDS = [234,4234,324,325543,2,34,546,74,567,568,568,89,364575,342,45,3456473,546,3456473]\n",
    "SEEDS = set(SEEDS)\n",
    "\n",
    "# Programmatically create the port objects for each of the port codes\n",
    "# for port_code in PORT_CODES:\n",
    "#     globals()[port_code] = port.Port() \n",
    "#     globals()[port_code].port_code = port_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95724805-1974-4443-b154-7e7fb86b32d0",
   "metadata": {},
   "source": [
    "To generate the cdf we can use np.norm.cdf(val, loc, scale) where location is the mean and scale is the standard deviation. \n",
    "To find the values for a certain percent we can use ppf from scipy stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603109a-2a3d-4a51-bdeb-40cd5115ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scheduled_departure_and_arrival_times(arrival, departure, trip_length, wanted_on_time_percent: float = 0.85) -> None:\n",
    "    trip_distribution = scipy.stats.norm(loc=MEAN_TRIP_TIME, scale=SD_TRIP_TIME)\n",
    "    loading_unloading_distribution = scipy.stats.norm(loc=MEAN_LOADING_UNLOADING_TIME, scale=SD_LOADING_UNLOADING_TIME)\n",
    "\n",
    "    current_time = 0\n",
    "    for i in range(1, trip_length):\n",
    "        # Add the time to get to the next port\n",
    "        current_time += trip_distribution.ppf(wanted_on_time_percent)\n",
    "\n",
    "        # Set the expected arrival time\n",
    "        arrival[i] = current_time\n",
    "\n",
    "        # Add the time to load and unload at that port\n",
    "        current_time += loading_unloading_distribution.ppf(wanted_on_time_percent)\n",
    "\n",
    "        #Set the expected departure time\n",
    "        departure[i] = current_time\n",
    "\n",
    "    for i in range(len(arrival)):\n",
    "        if type(arrival[i]) not in [int, float]:\n",
    "            arrival[i] = arrival[i].item()\n",
    "    for i in range(len(departure)):\n",
    "        if type(departure[i]) not in [int, float]:\n",
    "            departure[i] = departure[i].item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10919f80-1cde-4203-922a-f712c7682b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_arrival = [ 0 for i in range(4)]\n",
    "expected_departure = [0 for i in range(4)]\n",
    "\n",
    "generate_scheduled_departure_and_arrival_times(expected_arrival, expected_departure, 4, 0.70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b345d7-3c54-4708-a182-a0467e7ea983",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_to_count_as_late = 0\n",
    "lates_for_each = {}\n",
    "for ppf_value in range(1, 100, 1):\n",
    "    if (ppf_value % 10 == 0):\n",
    "        print(f\"PPF value: {ppf_value}\")\n",
    "    ppf_value = ppf_value / 100\n",
    "    lates_for_each[ppf_value] = []\n",
    "\n",
    "    expected_arrival = [ 0 for i in range(4)]\n",
    "    expected_departure = [0 for i in range(4)]\n",
    "    generate_scheduled_departure_and_arrival_times(expected_arrival, expected_departure, 4, ppf_value)\n",
    "    \n",
    "    for seed_value in range(100):\n",
    "        np.random.seed(seed_value)\n",
    "        # Most of the trips take the same amount of time independent of the ferry \n",
    "        # Makes every port to every other port take 20ish minutes. \n",
    "        # Times are made to be the same independent of direction.\n",
    "        TRIP_TIMES = {}\n",
    "        for port in PORT_CODES:\n",
    "            for other_port in [i for i in PORT_CODES if i != port]:\n",
    "                if other_port + port in TRIP_TIMES.keys():\n",
    "                    TRIP_TIMES[port+other_port] = TRIP_TIMES[other_port+port]\n",
    "                else:\n",
    "                    TRIP_TIMES[port + other_port] = np.random.normal(MEAN_TRIP_TIME, SD_TRIP_TIME)\n",
    "    \n",
    "        # Set the loading and unloading times\n",
    "        # Port + Current_time for every 5 step increment\n",
    "        LOADING_AND_UNLOADING_TIMES = {}\n",
    "        for port_code in PORT_CODES:\n",
    "            for time in range(0, MAX_SIMULATION_TIME, 5):\n",
    "                LOADING_AND_UNLOADING_TIMES[globals()[port_code] + time] = np.random.normal(MEAN_LOADING_UNLOADING_TIME, SD_LOADING_UNLOADING_TIME)\n",
    "        \n",
    "\n",
    "        \n",
    "        #Create a ferry object\n",
    "        QNW = ferryFile.Ferry()\n",
    "        QNW.ferry_name = \"Queen of New Westminster\"\n",
    "        QNW.ferry_code = \"QNW\"\n",
    "        QNW.ferry_capacity = 150\n",
    "        QNW.ferry_route = [TSA, SWB, TSA]\n",
    "        QNW.ferry_trip_time = TRIP_TIMES\n",
    "        QNW.set_expected_departure_times(expected_departure)\n",
    "        QNW.set_expected_arrival_times(expected_arrival)\n",
    "        QNW.loading_unloading_time = LOADING_AND_UNLOADING_TIMES\n",
    "        QNW.trips_required = 1\n",
    "        \n",
    "        QNW.trips_completed = 0\n",
    "        QNW.ferry_current_port_index = 0\n",
    "\n",
    "        QNW.print_stats_at_end = False\n",
    "\n",
    "        #QNW.validate_arrival_and_departure_times()\n",
    "    \n",
    "        current_time =0\n",
    "        event_queue = []\n",
    "        heapq.heapify(event_queue)\n",
    "        heapq.heappush(event_queue, (current_time, QNW))\n",
    "        \n",
    "    \n",
    "        current_event = heapq.heappop(event_queue)\n",
    "        while current_event[1].next_function != None:\n",
    "            current_time = current_event[0]\n",
    "    \n",
    "            next_event_time = current_event[1].next_function(current_time)\n",
    "            heapq.heappush(event_queue, (next_event_time, current_event[1]))\n",
    "    \n",
    "            current_event = heapq.heappop(event_queue)\n",
    "\n",
    "        lates_for_each[ppf_value].append(QNW.total_times_late(time_to_count_as_late))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(lates_for_each)\n",
    "for key in lates_for_each.keys():\n",
    "    data = lates_for_each[key]\n",
    "    times_late_to_arrive = [i[0] for i in data]\n",
    "    times_late_to_depart = [i[1] for i in data]\n",
    "\n",
    "    times_late_to_arrive = sum(times_late_to_arrive) / len(times_late_to_arrive)\n",
    "    times_late_to_depart = sum(times_late_to_depart) / len(times_late_to_depart)\n",
    "\n",
    "    lates_for_each[key] = [times_late_to_arrive, times_late_to_depart]\n",
    "#print(lates_for_each)\n",
    "\n",
    "plt.bar(lates_for_each.keys(), [lates_for_each[key][0] for key in lates_for_each.keys()], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb70b9a-7c2d-444f-8364-a56dbc120f52",
   "metadata": {},
   "source": [
    "Use pandas to load data from csv - Keep in mind to run this locally, and not upload the files to github or the jupyter server!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b8133-e6c4-438c-a7bb-38432960cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are some pandas examples\n",
    "data_file = None\n",
    "\n",
    "#Load data from file\n",
    "#all_data = pd.read_csv(file)\n",
    "data = {\"date_key\": [\"2024-04-0\" + str(i) for i in range(1,10)]}\n",
    "for i in range(10,28):\n",
    "    data[\"date_key\"].append(\"2024-04-\" + str(i))\n",
    "all_data = pd.DataFrame(data = data)\n",
    "\n",
    "#Seperate data by day of the week\n",
    "all_data['day_of_the_week'] = [datetime.datetime.strptime(i, \"%Y-%m-%d\").weekday() for i in all_data['date_key']]\n",
    "\n",
    "#Add route code\n",
    "all_data[\"route_id\"] = np.random.choice([5,9], all_data.shape[0])\n",
    "\n",
    "all_data.loc[all_data[\"route_id\"] == 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c48ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the year for the data from the file name\n",
    "def get_year(file_name: str) -> str:\n",
    "    print(file_name)\n",
    "    file_name = file_name.split(\"fy\")[1].strip(\".csv\").strip(\"fy\")\n",
    "    if \"_\" in file_name:\n",
    "        return file_name.split(\"_\")[0]\n",
    "    return file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71bde22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds a date key to the data  to make it easier to filter\n",
    "def add_date_key(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    #If the data set is analyze or event then there is a \"date_key\" column exclusive of the end date\n",
    "    if \"date_key\" in data.columns: \n",
    "        #Date is in the format of YYYY-MM-DD\n",
    "        data[\"date_key\"] = pd.to_datetime(data[\"date_key\"])\n",
    "\n",
    "        date = (data.loc[data[\"sailing_id\"] == \"LONG202304071800\"][\"date_key\"].values[0])\n",
    "        print(type(date))\n",
    "        print(date.astype(str)[:10])\n",
    "        \n",
    "\n",
    "    #If it is injest then there is a \"Sched Dept Ts\" column\n",
    "    elif \"Sched Dept Ts\" in data.columns: \n",
    "        #Date is the format of YYYY-MM-DD-HH-MM-SS\n",
    "        data[\"date_key\"] = pd.to_datetime(data[\"Sched Dept Ts\"])\n",
    "\n",
    "    #Throw an error to show that we have a data set without the proper columns\n",
    "    else: \n",
    "        print(\"A data set was passed that did not follow any of the formats\")\n",
    "        assert(False)\n",
    "        \n",
    "\n",
    "def filter_by_dates(data: pd.DataFrame, start_date: str | np.datetime64, end_date: str | np.datetime64) -> pd.DataFrame:\n",
    "    # Filter the data set by the start and end date and return the new data set\n",
    "    if type(start_date) is str:\n",
    "        start_date = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    if type(end_date) is str:\n",
    "        end_date = datetime.datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    \n",
    "\n",
    "\n",
    "    assert(start_date <= end_date), \"Start date must be before end date\"\n",
    "\n",
    "    new_data = data.loc[data[\"date_key\"] >= start_date]\n",
    "    new_data = new_data.loc[new_data[\"date_key\"] < end_date]\n",
    "\n",
    "    return new_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbadd73-9ccc-47d8-bbf0-ff776207ee5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "assert(load_dotenv())\n",
    "\n",
    "ANALYZE_FOLDER = os.getenv(\"SAILING_ANALYZE_FOLDER\")\n",
    "INGEST_FOLDER = os.getenv(\"SAILING_INGEST_FOLDER\")\n",
    "EVENT_FOLDER = os.getenv(\"SAILING_EVENT_FOLDER\")\n",
    "\n",
    "files = os.listdir(ANALYZE_FOLDER) \n",
    "all_analyze_data = pd.concat(map(pd.read_csv, [ANALYZE_FOLDER + \"/\" + file for file in files]))\n",
    "all_analyze_data.dropna(how='all', inplace=True, ignore_index=True)\n",
    "\n",
    "def convert_to_epoch(date: str):\n",
    "    dt_obj = datetime.datetime.strptime(date, \"%Y-%m-%d %H:%M\")\n",
    "    return int(dt_obj.timestamp())\n",
    "\n",
    "all_analyze_data[\"arrival_actual_datetime\"] = all_analyze_data[\"arrival_actual_datetime\"].apply(convert_to_epoch)\n",
    "all_analyze_data[\"departure_actual_datetime\"] = all_analyze_data[\"departure_actual_datetime\"].apply(convert_to_epoch)\n",
    "\n",
    "all_analyze_data[\"sailing_time\"] = (all_analyze_data[\"arrival_actual_datetime\"] - all_analyze_data[\"departure_actual_datetime\"])/60\n",
    "add_date_key(all_analyze_data)\n",
    "assert(all_analyze_data.loc[all_analyze_data[\"sailing_time\"] <= 0].empty) #Sanity check that none of the boats are time travellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72499e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(all_analyze_data[\"date_key\"].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c19802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the inport time for the vessel\n",
    "def get_in_port_time(data: pd.DataFrame, sailing_id: str) -> int:\n",
    "    #Get the information for the sailing id and data from the day\n",
    "    sailing_info = data.loc[data[\"sailing_id\"] == sailing_id]\n",
    "    if len(sailing_info.index) != 1:\n",
    "        sailing_info = sailing_info.loc[sailing_info[\"route_id\"] == 5]\n",
    "    assert len(sailing_info.index) == 1, f\"There should only be one sailing id in the data set: {sailing_id}\"\n",
    "    data_for_the_day = filter_by_dates(data, sailing_info[\"date_key\"].values[0], sailing_info[\"date_key\"].values[0] + np.timedelta64(1, 'D'))\n",
    "    \n",
    "    arrival_port_id = sailing_info[\"arr_port_id\"].values[0]\n",
    "\n",
    "    #Filter by port\n",
    "    data_for_the_day = data_for_the_day.loc[data_for_the_day[\"dep_port_id\"] == arrival_port_id]\n",
    "    if len(data_for_the_day.index) == 0:\n",
    "        print(\"No data for the day\")\n",
    "        return -1\n",
    "    #Filter by time\n",
    "    sailing_arrival = sailing_info[\"arrival_actual_datetime\"].values[0]\n",
    "    data_for_the_day = data_for_the_day.loc[data_for_the_day[\"departure_actual_datetime\"] > sailing_arrival]\n",
    "    if len(data_for_the_day.index) == 0:\n",
    "        print(\"No data for the day\")\n",
    "        return -1\n",
    "    #Get what should have been the arrival\n",
    "    time_wanted = data_for_the_day[\"departure_actual_datetime\"].min()\n",
    "    arrival = data_for_the_day.loc[data_for_the_day[\"departure_actual_datetime\"] == time_wanted]\n",
    "\n",
    "\n",
    "    arri_time = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=int(sailing_info[\"arrival_actual_datetime\"].values[0].astype(str)))\n",
    "    dept_time = datetime.datetime(1970, 1, 1) + datetime.timedelta(seconds=int(arrival[\"departure_actual_datetime\"].values[0].astype(str)))\n",
    "\n",
    "\n",
    "\n",
    "    # Compute difference\n",
    "    time_diff = dept_time - arri_time\n",
    "    return time_diff.total_seconds() / 60\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if (os.path.exists(\"in_port_time_at_destination.pkl\")):\n",
    "    all_analyze_data[\"in_port_time_at_destination\"] = pd.read_pickle(\"in_port_time_at_destination.pkl\")\n",
    "else:\n",
    "    all_analyze_data[\"in_port_time_at_destination\"] = all_analyze_data[\"sailing_id\"].apply(\n",
    "        lambda x: get_in_port_time(all_analyze_data, x)\n",
    "    )\n",
    "    all_analyze_data[\"in_port_time_at_destination\"].to_pickle(\"in_port_time_at_destination.pkl\")\n",
    "\n",
    "\n",
    "all_analyze_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a993de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analyze_data.loc[all_analyze_data[\"sailing_id\"] == \"OB202208210710\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1236d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = all_analyze_data.loc[all_analyze_data[\"sailing_id\"] == \"OB202304071815\"]\n",
    "x[\"date_key\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d0d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(EVENT_FOLDER)\n",
    "eventData = pd.concat(map(pd.read_csv, [EVENT_FOLDER + \"/\" + file for file in files])) #Read all the files and concatenate them into one dataframe\n",
    "eventData.dropna(how=\"all\", inplace=True, ignore_index=True)\n",
    "\n",
    "arr_ports = [int(i) for i in  eventData[\"arr_port_id\"].unique()]\n",
    "dep_ports = [int(i) for i in  eventData[\"dep_port_id\"].unique()]\n",
    "\n",
    "ports = set(arr_ports + dep_ports)\n",
    "\n",
    "portID_to_port_map = {}\n",
    "for port in ports: \n",
    "    #Check arrivals \n",
    "    port_abb_arr = eventData.loc[eventData[\"arr_port_id\"] == port][\"arr_port_abb\"].unique()\n",
    "\n",
    "    #Check departures\n",
    "    port_abb_dep = eventData.loc[eventData[\"dep_port_id\"] == port][\"dep_port_abb\"].unique()\n",
    "\n",
    "    #Verify that one the abbreviation was found for atleast 1 and if it was found for only one assign it to both.\n",
    "    if len(port_abb_dep) == 0 and len(port_abb_arr) == 0: \n",
    "        print(f\"Could not find the abbreviation for {port}.\")\n",
    "        continue\n",
    "    elif len(port_abb_dep) == 0: \n",
    "        port_abb_dep = port_abb_arr\n",
    "    elif len(port_abb_arr) == 0:\n",
    "        port_abb_arr = port_abb_dep\n",
    "\n",
    "    port_abb_arr = port_abb_arr[0]\n",
    "    port_abb_dep = port_abb_dep[0]\n",
    "    \n",
    "    \n",
    "    assert(port_abb_dep == port_abb_arr)\n",
    "\n",
    "    if port in portID_to_port_map.keys():\n",
    "        assert(portID_to_port_map[port] == port_abb_arr) #Sanity check\n",
    "    else: \n",
    "        portID_to_port_map[port] = port_abb_arr\n",
    "\n",
    "    \n",
    "def portID_to_abb(portID:int | str) -> str: \n",
    "    if type(portID) is str: \n",
    "        portID = int(portID)\n",
    "\n",
    "    return portID_to_port_map[portID]\n",
    "\n",
    "def abb_to_portID(portAbb:str) -> int | str:\n",
    "    for portID in portID_to_port_map.keys():\n",
    "        if portID_to_port_map[portID] == portAbb:\n",
    "            return str(portID)\n",
    "        \n",
    "    assert False, f\"Port abbreviation {portAbb} not found in portID_to_port_map\"\n",
    "\n",
    "portID_to_port_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d1971",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIGURES_DIR = \"figures\"\n",
    "\n",
    "#Seperate data by route\n",
    "Route5_data = all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 5)]\n",
    "Route9_data = all_analyze_data.loc[all_analyze_data[\"route_id\"] == 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dfd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_port_from_route(route_data, route_leg_id, arrival: bool = False, departure: bool = False):\n",
    "    if arrival and departure:\n",
    "        raise ValueError(\"Both arrival and departure cannot be true\")\n",
    "    if not arrival and not departure:\n",
    "        raise ValueError(\"Either arrival or departure must be true\")\n",
    "    if arrival:\n",
    "        key = \"arr_port_id\"\n",
    "    else:\n",
    "        assert(departure)\n",
    "        key = \"dep_port_id\"\n",
    "\n",
    "\n",
    "    port_id = route_data[route_data[\"route_leg_id\"] == route_leg_id][key].unique()\n",
    "    assert(len(port_id) == 1)\n",
    "    port_id = port_id[0]\n",
    "    return port_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e90867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make histograms for each of the legs with all of the vessels to see the general distribution of sailing times\n",
    "def make_histograms_for_legs(route_data: pd.DataFrame) -> None:\n",
    "    route_vessels = route_data[\"vessel_id\"].unique()\n",
    "    route_legs = route_data[\"route_leg_id\"].unique()\n",
    "\n",
    "\n",
    "    for route_leg_id in route_legs: \n",
    "        #Get the arrival and departure port ids\n",
    "        arrival_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, arrival=True))))\n",
    "        departure_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, departure=True))))\n",
    "        route_id = route_leg_id.split(\"-\")[0]\n",
    "\n",
    "        route_figure = plt.figure()\n",
    "        plt.title(f\"Route {route_leg_id.split('-')[0]}, Leg {departure_port_id} to {arrival_port_id}\")\n",
    "        plt.xlabel(\"Sailing Time (m)\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "\n",
    "        #Get the min x and max x for the histogram\n",
    "        min_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].min()\n",
    "        max_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].max()\n",
    "\n",
    "        \n",
    "        for vessel in route_vessels: \n",
    "            #verify the vessel has sailed on the route otherwise skip\n",
    "            if route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)].empty:\n",
    "                continue\n",
    "\n",
    "            #plot the histogram\n",
    "            route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)][\"sailing_time\"].hist(bins=25, alpha=0.3, label=f\"Vessel {vessel}\", range=[min_x, max_x])\n",
    "\n",
    "        #format the plot\n",
    "        plt.grid(False)\n",
    "        plt.legend()\n",
    "\n",
    "        FOLDER = f\"route_{route_id}_sailing_times\"\n",
    "        if (FOLDER not in os.listdir(FIGURES_DIR)):\n",
    "            os.mkdir(f\"{FIGURES_DIR}/{FOLDER}\")\n",
    "        plt.savefig(f\"{FIGURES_DIR}/{FOLDER}/{departure_port_id + arrival_port_id}_histogram.png\")\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "make_histograms_for_legs(Route5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e8cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make plots for the sailing times on each route for each vessel to check if the timing is consistent across vessels.\n",
    "def make_subplot_histograms_for_legs(route_data: pd.DataFrame) -> None:\n",
    "    route_vessels = route_data[\"vessel_id\"].unique()\n",
    "    route_legs = route_data[\"route_leg_id\"].unique()\n",
    "\n",
    "    for route_leg_id in route_legs: \n",
    "        #Get the arrival, departure port ids and the route id\n",
    "        arrival_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, arrival=True))))\n",
    "        departure_port_id = portID_to_abb(str(int(get_port_from_route(route_data, route_leg_id, departure=True))))\n",
    "        route_id = route_leg_id.split(\"-\")[0]\n",
    "\n",
    "        #Create the shared plot\n",
    "        route_figure = plt.figure()\n",
    "        plt.suptitle(f\"Route {route_id}, Leg {departure_port_id} to {arrival_port_id}\")\n",
    "\n",
    "        #Get the min x and max x for the histogram\n",
    "        min_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].min()\n",
    "        max_x = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"sailing_time\"].max()\n",
    "\n",
    "        for vessel in route_vessels: \n",
    "            #verify the vessel has sailed on the route\n",
    "            if route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)].empty:\n",
    "                continue\n",
    "\n",
    "\n",
    "            #Determine the subplot location\n",
    "            unique_vessels_for_route = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"vessel_id\"].nunique()\n",
    "            index_array_for_vessel_on_route = route_data.loc[route_data[\"route_leg_id\"] == route_leg_id, \"vessel_id\"].unique()\n",
    "            plt.subplot(1 if unique_vessels_for_route <= 3 else 2, \n",
    "                        unique_vessels_for_route if unique_vessels_for_route <= 3 else 3, \n",
    "                        np.where(index_array_for_vessel_on_route == vessel)[0][0] + 1)\n",
    "            \n",
    "            #Plot the data\n",
    "            route_data.loc[(route_data[\"vessel_id\"] == vessel) & (route_data[\"route_leg_id\"] == route_leg_id)][\"sailing_time\"].hist(\n",
    "                bins=20, alpha=0.5, label=f\"Vessel {vessel}\", range=[min_x, max_x])\n",
    "\n",
    "            #Set the title, labels, and grid\n",
    "            plt.title(f\"Vessel {vessel}\")\n",
    "            plt.xlabel(\"Sailing Time (m)\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.grid(False)\n",
    "            \n",
    "        \n",
    "        plt.tight_layout()\n",
    "        FOLDER = f\"route_{route_id}_sailing_times_subplots\"\n",
    "        if (FOLDER not in os.listdir(FIGURES_DIR)):\n",
    "            os.mkdir(f\"{FIGURES_DIR}/{FOLDER}\")\n",
    "        plt.savefig(f\"{FIGURES_DIR}/{FOLDER}/{departure_port_id+arrival_port_id}.png\")\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "make_subplot_histograms_for_legs(Route5_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df61c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 9) & (all_analyze_data[\"route_leg_id\"].apply(lambda x: x.split(\"-\")[0]) == \"5\")]\n",
    "all_analyze_data.loc[(all_analyze_data[\"route_id\"] == 5) & (all_analyze_data[\"route_leg_id\"].apply(lambda x: x.split(\"-\")[0]) == \"9\")]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7295bc",
   "metadata": {},
   "source": [
    "##Simple sim without ferries mattering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003f77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sailing_times(route): \n",
    "    return [all_analyze_data.loc[(all_analyze_data[\"sailing_leg_id\"] == int(leg))][\"sailing_time\"] for leg in route]\n",
    "\n",
    "def get_in_port_times(route): \n",
    "    return [all_analyze_data.loc[(all_analyze_data[\"sailing_leg_id\"] == int(leg))][\"in_port_time_at_destination\"] for leg in route]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a27026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "route = [\"SWB,STB\", \"STB,VB\", \"VB,SWB\", \"SWB,OB\", \"OB,SWB\", \"SWB,VB\", \"VB,OB\", \"OB,SWB\"] #Example route\n",
    "OVER_TIME = 800\n",
    "ROUTE_START_TIME = 305\n",
    "route = [i.split(\",\") for i in route]\n",
    "route = [abb_to_portID(i[0]) + abb_to_portID(i[1]) for i in route]\n",
    "\n",
    "\n",
    "sched_dept_times = [[5,5], [6,25], [7, 5], [8,15], [9,10]] #[hour, minute]\n",
    "sched_dept_times = [i[0] * 60 + i[1] for i in sched_dept_times]\n",
    "\n",
    "\n",
    "def gen_dept_times(sailing_times, in_port_times,start_time, quantile: float = 0.8, buffer: int = 0) -> list:\n",
    "    sailing_times = [np.quantile(i, quantile) for i in sailing_times]\n",
    "    in_port_times = [np.quantile(i, quantile) for i in in_port_times]\n",
    "\n",
    "    sched_dept_times = [start_time]\n",
    "    for i in range(1, len(sailing_times)):\n",
    "        time_for_leg = sailing_times[i-1] + in_port_times[i-1] + buffer\n",
    "        sched_dept_times.append(sched_dept_times[i-1] + time_for_leg)\n",
    "\n",
    "\n",
    "    return sched_dept_times\n",
    "\n",
    "\n",
    "class simulation_result:\n",
    "     def __init__(self, actual_dept_times, seed_value, route, sched_dept_times):\n",
    "        self.actual_dept_times = actual_dept_times\n",
    "        self.seed_value = seed_value\n",
    "        self.route = route\n",
    "        self.sched_dept_times = sched_dept_times\n",
    "        self.time_end = 0\n",
    "\n",
    "\n",
    "def simulate(route, sched_dept_times, seed_value = 0, sailing_times = None, in_port_times =None) -> simulation_result:\n",
    "    if sailing_times == None: sailing_times = [all_analyze_data.loc[(all_analyze_data[\"sailing_leg_id\"] == int(leg))][\"sailing_time\"] for leg in route]\n",
    "    if in_port_times == None: in_port_times = [all_analyze_data.loc[(all_analyze_data[\"sailing_leg_id\"] == int(leg))][\"in_port_time_at_destination\"] for leg in route]\n",
    "\n",
    "\n",
    "    #Seed each run to get the same results\n",
    "    np.random.seed(seed_value)\n",
    "\n",
    "    time = sched_dept_times[0]\n",
    "    actual_dept = [time]\n",
    "\n",
    "    for i in range(1, len(sched_dept_times)):\n",
    "        time_for_leg = np.random.choice(sailing_times[i-1]) + np.random.choice(in_port_times[i-1])\n",
    "        time += time_for_leg\n",
    "        if time < sched_dept_times[i]:\n",
    "            time = sched_dept_times[i]\n",
    "        actual_dept.append(time)\n",
    "\n",
    "    result = simulation_result(actual_dept, seed_value, route, sched_dept_times)\n",
    "    result.time_end = get_the_end_time(actual_dept, route, sailing_times)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def calculate_on_time_performance_per_run(sceduled_dept_times, actual_departure_times) -> float: \n",
    "    LATE_THRESHOLD = 10 #minutes\n",
    "    assert(len(sceduled_dept_times) == len(actual_departure_times)), \"Scheduled and actual departure times must be the same length\"\n",
    "    times_late = 0 \n",
    "    for i in range(len(sceduled_dept_times)):\n",
    "        #print(f\"Scheduled: {sceduled_dept_times[i]} Actual: {actual_departure_times[i]}\")\n",
    "        if actual_departure_times[i] > (sceduled_dept_times[i] + LATE_THRESHOLD):\n",
    "            times_late += 1\n",
    "\n",
    "    return (1 - (times_late / len(sceduled_dept_times))) * 100 #percentage of times late\n",
    "\n",
    "def run_simulations(route, number_of_simulations, sched_dept_times):\n",
    "    # Run a bunch of simulations \n",
    "    all_runs = []\n",
    "    np.random.seed(0)\n",
    "    SEEDS = np.random.choice(range(1, 64 * 1000), size=number_of_simulations, replace=False)\n",
    "    for seed_value in SEEDS:\n",
    "        data = simulate(route, sched_dept_times, seed_value=seed_value)\n",
    "        all_runs.append(data)\n",
    "    \n",
    "    return all_runs\n",
    "\n",
    "\n",
    "def plot_histogram_of_departure_times(all_runs, sched_dept_times, name, save_to_file_of_name = False) -> None:\n",
    "    all_actual_dept_times = np.array([i.actual_dept_times for i in all_runs])\n",
    "    indices = all_actual_dept_times.shape[1]\n",
    "\n",
    "    bin_width = 2  \n",
    "    bins = np.arange(all_actual_dept_times.min(), all_actual_dept_times.max() + bin_width, bin_width) \n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    #plot the histogram for each leg\n",
    "    for i in range(indices):\n",
    "        Leg_name = portID_to_abb(all_runs[0].route[i][:4]) +\",\"+ portID_to_abb(all_runs[0].route[i][4:])\n",
    "        plt.hist(all_actual_dept_times[:, i], bins=bins, alpha=0.5, label=f'Leg {Leg_name}', edgecolor='black')\n",
    "\n",
    "    locs, labels = plt.xticks()\n",
    "    labels = [str(int((int(item.get_text()) - int(item.get_text()) % 60) / 60)) + \"h\" + str(int(item.get_text()) % 60) + \"m\" for item in labels]\n",
    "    plt.xticks(locs, labels)\n",
    "    \n",
    "    #plot for the end time\n",
    "    plt.hist([i.time_end for i in all_runs], bins=bins, alpha=0.5, label='End Time', color='black', edgecolor='red')\n",
    "    plt.axvline(np.mean([i.time_end for i in all_runs]), color='black', linestyle='dashed', linewidth=1, label='Mean End Time')\n",
    "    plt.axvline(OVER_TIME, color='green', linestyle='dashed', linewidth=1, label='Over Time')\n",
    "\n",
    "\n",
    "    #plot the late point and scheduled departure times\n",
    "    for i ,dept_time in enumerate(sched_dept_times):\n",
    "        #only make a label for the first\n",
    "        if i == 0: \n",
    "            plt.axvline(dept_time, color='blue', linestyle='dashed', linewidth=1, label=f'Scheduled Departure')\n",
    "            plt.axvline(dept_time + 10, color='red', linestyle='dashed', linewidth=1, label=f'Late')\n",
    "        else: \n",
    "            plt.axvline(dept_time, color='blue', linestyle='dashed', linewidth=1)\n",
    "            plt.axvline(dept_time + 10, color='red', linestyle='dashed', linewidth=1)\n",
    "\n",
    "    plt.title(f\"Departure Times for {name}\")\n",
    "    plt.xlabel('Departure Time (m)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "\n",
    "    FOLDER = \"departure_time_histograms\"\n",
    "    if (FOLDER not in os.listdir(FIGURES_DIR)):\n",
    "        os.mkdir(f\"{FIGURES_DIR}/{FOLDER}\")\n",
    "\n",
    "    if save_to_file_of_name:\n",
    "        plt.savefig(f\"{FIGURES_DIR}/{FOLDER}/{name}_histogram.png\")\n",
    "    plt.show()\n",
    "\n",
    "#TODO: get the end time\n",
    "TIME_NEEDED_AT_THE_END_OF_THE_ROUTE = 10 #minutes\n",
    "def get_the_end_time(actual_dept, route, sailing_times) -> int: \n",
    "    return actual_dept[-1] + np.random.choice(sailing_times[-1]) + TIME_NEEDED_AT_THE_END_OF_THE_ROUTE #10 minutes buffer\n",
    "\n",
    "# Get the average ontime performance for each leg and the whole route\n",
    "def get_average_on_time_performance(all_runs: list) -> float:\n",
    "    route_length = len(all_runs[0].actual_dept_times)\n",
    "    on_time_performance = [0 for i in range(route_length)]\n",
    "    for i in range(route_length):\n",
    "        on_time_performance[i] = sum([1 for n in range(len(all_runs)) if all_runs[n].actual_dept_times[i] <= (all_runs[n].sched_dept_times[i] + 10)]) / len(all_runs) * 100\n",
    "    #print(f\"Avg on_time performance per leg: {on_time_performance}\")\n",
    "\n",
    "\n",
    "    return on_time_performance, np.mean(on_time_performance)\n",
    "#get_average_on_time_performance(all_runs)\n",
    "\n",
    "\n",
    "sailing_times = get_sailing_times(route)\n",
    "in_port_times = get_in_port_times(route)\n",
    "per_quantile_results = {}\n",
    "per_quantile_results_with_buffer = {}\n",
    "\n",
    "per_quantile_results_per_leg = {}\n",
    "per_quantile_results_per_leg_with_buffer = {}\n",
    "\n",
    "\n",
    "\n",
    "def thread_function(quantile): \n",
    "    quantile = quantile / 100\n",
    "\n",
    "    sched_dept_times = gen_dept_times(sailing_times, in_port_times, ROUTE_START_TIME, quantile=quantile)\n",
    "    all_runs = run_simulations(route, 100, sched_dept_times)\n",
    "    plot_histogram_of_departure_times(all_runs, sched_dept_times, f\"Quantile_{int(quantile* 100)}\", save_to_file_of_name=True)\n",
    "\n",
    "    on_time_performance, avg_on_time_performance = get_average_on_time_performance(all_runs)\n",
    "    #print(f\"Quantile: {quantile} Avg on time performance: {avg_on_time_performance}\")\n",
    "\n",
    "    per_quantile_results[quantile] = avg_on_time_performance\n",
    "    per_quantile_results_per_leg[quantile] = on_time_performance\n",
    "\n",
    "    sched_dept_times = gen_dept_times(sailing_times, in_port_times, ROUTE_START_TIME, quantile=quantile, buffer=5)\n",
    "    all_runs = run_simulations(route, 100, sched_dept_times)\n",
    "    plot_histogram_of_departure_times(all_runs, sched_dept_times, f\"Quantile_{int(quantile* 100)}_B5\", save_to_file_of_name=True)\n",
    "\n",
    "    on_time_performance, avg_on_time_performance = get_average_on_time_performance(all_runs)\n",
    "    #print(f\"Quantile: {quantile} Avg on time performance: {avg_on_time_performance}\")\n",
    "\n",
    "    per_quantile_results_with_buffer[quantile] = avg_on_time_performance\n",
    "    per_quantile_results_per_leg_with_buffer[quantile] = on_time_performance\n",
    "\n",
    "for quantile in range(20, 101, 20):\n",
    "    thread_function(quantile)\n",
    "\n",
    "\n",
    "# plot a line graph of the average on time performance for each quantile\n",
    "plt.figure()\n",
    "plt.plot(per_quantile_results.keys(), per_quantile_results.values())\n",
    "plt.plot(per_quantile_results_with_buffer.keys(), per_quantile_results_with_buffer.values())\n",
    "plt.legend([\"Without Buffer\", \"With Buffer 5m\"])\n",
    "plt.title(\"Average On Time Performance vs Quantile\")\n",
    "plt.xlabel(\"Quantile\")\n",
    "plt.ylabel(\"Average On Time Performance\")\n",
    "plt.grid(True)\n",
    "plt.savefig(f\"{FIGURES_DIR}/Quantile_vs_Average_On_Time_Performance.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49237a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a line graph for each of the legs on time performance\n",
    "for quantile in per_quantile_results_per_leg.keys():\n",
    "    plt.figure()\n",
    "    plt.plot(per_quantile_results_per_leg[quantile])\n",
    "    plt.plot(per_quantile_results_per_leg_with_buffer[quantile])\n",
    "    plt.legend([\"Without Buffer\", \"With Buffer 5m\"])\n",
    "    plt.title(f\"Average On Time Performance vs Leg for Quantile {quantile}\")\n",
    "    plt.xlabel(\"Leg\")\n",
    "    plt.ylabel(\"Average On Time Performance\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{FIGURES_DIR}/Quantile_{quantile}_vs_Average_On_Time_Performance.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
